#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Build and test

on:
  workflow_call:
    inputs:
      java:
        required: false
        type: string
        default: 8
      branch:
        description: Branch to run the build against
        required: false
        type: string
        default: master
      hadoop:
        description: Hadoop version to run with. HADOOP_PROFILE environment variable should accept it.
        required: false
        type: string
        default: hadoop3
      envs:
        description: Additional environment variables to set when running the tests. Should be in JSON format.
        required: false
        type: string
        default: '{}'
      jobs:
        description: >-
          Jobs to run, and should be in JSON format. The values should be matched with the job's key defined
          in this file, e.g., build. See precondition job below.
        required: false
        type: string
        default: ''
jobs:
  precondition:
    name: Check changes
    runs-on: ubuntu-20.04
    env:
      GITHUB_PREV_SHA: ${{ github.event.before }}
    outputs:
      required: ${{ steps.set-outputs.outputs.required }}
      image_url: >-
        ${{
          (inputs.branch == 'master' && steps.infra-image-outputs.outputs.image_url)
          || 'dongjoon/apache-spark-github-action-image:20220207'
        }}
    steps:
    - name: Checkout Spark repository
      uses: actions/checkout@v2
      with:
        fetch-depth: 0
        repository: apache/spark
        ref: ${{ inputs.branch }}
    - name: Sync the current branch with the latest in Apache Spark
      if: github.repository != 'apache/spark'
      run: |
        echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
    - name: Check all modules
      id: set-outputs
      run: |
        if [ -z "${{ inputs.jobs }}" ]; then
          # is-changed.py is missing in branch-3.2, and it might run in scheduled build, see also SPARK-39517
          pyspark=true; sparkr=true; tpcds=true; docker=true;
          if [ -f "./dev/is-changed.py" ]; then
            pyspark_modules=`cd dev && python -c "import sparktestsupport.modules as m; print(','.join(m.name for m in m.all_modules if m.name.startswith('pyspark')))"`
            pyspark=`./dev/is-changed.py -m $pyspark_modules`
            sparkr=`./dev/is-changed.py -m sparkr`
            tpcds=`./dev/is-changed.py -m sql`
            docker=`./dev/is-changed.py -m docker-integration-tests`
          fi
          # 'build', 'scala-213', and 'java-11-17' are always true for now.
          # It does not save significant time and most of PRs trigger the build.
          precondition="
            {
              \"build\": \"true\",
              \"pyspark\": \"$pyspark\",
              \"sparkr\": \"$sparkr\",
              \"tpcds-1g\": \"$tpcds\",
              \"docker-integration-tests\": \"$docker\",
              \"scala-213\": \"true\",
              \"java-11-17\": \"true\",
              \"lint\" : \"true\",
              \"k8s-integration-tests\" : \"true\",
            }"
          echo $precondition # For debugging
          # GitHub Actions set-output doesn't take newlines
          # https://github.community/t/set-output-truncates-multiline-strings/16852/3
          precondition="${precondition//$'\n'/'%0A'}"
          echo "::set-output name=required::$precondition"
        else
          # This is usually set by scheduled jobs.
          precondition='${{ inputs.jobs }}'
          echo $precondition # For debugging
          precondition="${precondition//$'\n'/'%0A'}"
          echo "::set-output name=required::$precondition"
        fi
    - name: Generate infra image URL
      id: infra-image-outputs
      run: |
        # Convert to lowercase to meet Docker repo name requirement
        REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
        IMG_NAME="apache-spark-ci-image:${{ inputs.branch }}-${{ github.run_id }}"
        IMG_URL="ghcr.io/$REPO_OWNER/$IMG_NAME"
        echo ::set-output name=image_url::$IMG_URL

  k8s-c-tests:
    needs: precondition
    if: fromJson(needs.precondition.outputs.required).k8s-integration-tests == 'true'
    name: Run Spark on Kubernetes c test
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: ${{ inputs.branch }}
      - name: Sync the current branch with the latest in Apache Spark
        if: github.repository != 'apache/spark'
        run: |
          echo "APACHE_SPARK_REF=$(git rev-parse HEAD)" >> $GITHUB_ENV
          git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m "Merged commit" --allow-empty
      - name: Cache Scala, SBT and Maven
        uses: actions/cache@v2
        with:
          path: |
            build/apache-maven-*
            build/scala-*
            build/*.jar
            ~/.sbt
          key: build-${{ hashFiles('**/pom.xml', 'project/build.properties', 'build/mvn', 'build/sbt', 'build/sbt-launch-lib.bash', 'build/spark-build-info') }}
          restore-keys: |
            build-
      - name: Cache Coursier local repository
        uses: actions/cache@v2
        with:
          path: ~/.cache/coursier
          key: k8s-c-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}
          restore-keys: |
            k8s-c-coursier-
      - name: Install Java ${{ inputs.java }}
        uses: actions/setup-java@v1
        with:
          java-version: ${{ inputs.java }}
#      - name: start kind cluster
#        uses: helm/kind-action@v1.2.0
      - name: start minikube
        run: |
          # See more in "Installation" https://minikube.sigs.k8s.io/docs/start/
          curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
          sudo install minikube-linux-amd64 /usr/local/bin/minikube
          # Github Action limit cpu:2, memory: 6947MB, limit to 2U6G for better resource statistic
          minikube start --cpus 2 --memory 6144
      - name: Print K8S pods and nodes info
        run: |
          echo "info: waiting for all pods in kube-system namespace to be ready"
          kubectl -n kube-system wait --for=condition=Ready pods --all
          echo "info: print cluster information"
          kubectl describe nodes
          kubectl get pods -n kube-system
      - name: Encode chaos-mesh action
        run: |
          echo CFG_BASE64=$(base64 -w 0 $GITHUB_WORKSPACE/dev/infra/chaos/test.yaml) >> $GITHUB_ENV
      - name: Run chaos mesh action
        uses: chaos-mesh/chaos-mesh-action@master
        env:
          CFG_BASE64: ${{ env.CFG_BASE64 }}
          CHAOS_MESH_VERSION: v2.4.0
      - name: Run Spark on K8S integration test (With driver cpu 0.5, executor cpu 0.2 limited)
        run: |
            # Prepare PV test
            PVC_TMP_DIR=$(mktemp -d)
            export PVC_TESTS_HOST_PATH=$PVC_TMP_DIR
            export PVC_TESTS_VM_PATH=$PVC_TMP_DIR
            minikube mount ${PVC_TESTS_HOST_PATH}:${PVC_TESTS_VM_PATH} --gid=0 --uid=185 &
            kubectl create clusterrolebinding serviceaccounts-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts || true
            eval $(minikube docker-env)
            # - Exclude Volcano test (-Pvolcano), batch jobs need more CPU resource
            build/sbt -Psparkr -Pkubernetes -Pkubernetes-integration-tests -Dspark.kubernetes.test.driverRequestCores=0.5 -Dspark.kubernetes.test.executorRequestCores=0.2 "kubernetes-integration-tests/test"
      - name: Upload Spark on K8S integration tests log files
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: spark-on-kubernetes-it-log
          path: "**/target/integration-tests.log"